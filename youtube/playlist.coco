"""
Download all the songs inside a YouTube playlist and for each song:

- Add metadata (title, artist, lyrics, etc)
- Adjust gain
- Add artwork

"""
import sys
import json
import subprocess
from pathlib import Path
import urllib.parse

import webvtt

import settings

here = Path(__file__).parent

download_dir = here / 'downloads'
output_dir = here / 'output'
json_file = here / 'youtube.json'

def main():
  playlist_url = if len(sys.argv) > 1 then sys.argv[1] else settings.YOUTUBE_PLAYLIST
  download_songs(playlist_url)
  generate_json()
  add_metadata()

def download_songs(url):
    """
    If downloads folder doesn't have any mp4 files, download files from given YouTube URL.
    """
    files = download_dir.glob('*.mp4') |> list
    if len(files) > 0:
      return

    subprocess.run([
      'yt-dlp',
      '--write-thumbnail',
      '--write-info-json',
      '--sub-langs', 'all', '--write-subs',
      '--format', '[ext=mp4]',
      '--output', f'{download_dir}/%(title)s-%(id)s.%(ext)s',
      url,
    ])

    # Nicely format the info.json files
    for info_file in download_dir.glob('*.info.json'):
      slurp_json(info_file) |> spit_json$(info_file)

    print(f'Files downloaded in {download_dir}')


def generate_json():
    """
    If youtube.json doesn't exist, read .info.json files from downloads directory and generate it
    """
    if json_file.exists():
        return

    def info_to_meta(info_objects):
      for info in info_objects:
        yield dict(
          title=info['title'],
          artist=info['channel'],
          channel=info['channel'],
          album='',
          genre='Pop',  # just a placeholder
          link=f"https://youtu.be/{info['id']}",
          path=info['path'],
          start=None,
          end=None,
        )

    get_info_objects() |> info_to_meta |> list |> spit_json$(json_file)
    print('\nGenerated youtube.json, edit it, and run `make playlist` again!')

def print_search_urls():
    print('\nLyrics search strings:')
    for meta in slurp_json(json_file):
      query = f"{meta['title']} {meta['artist']} 歌词".replace(' ', '+')
      print(f"https://www.google.com/search?q={query}")

def add_metadata():
    """
    If youtube.json file exists, iterate over its entries and generate corresponding .m4a files in output directory
    """
    if not json_file.exists():
      return

    for meta in slurp_json(json_file):
      input_file = Path(meta['path'])
      info_file = input_file.with_suffix('.info.json')
      info = slurp_json(info_file)
      info.update(meta)

      file_name = f"{meta['artist']}  {meta['title']}.m4a".replace('/', '_')
      output_file = output_dir / file_name

      add_metadata_for_file(input_file, output_file, info)

    print_search_urls()


def add_metadata_for_file(input_file, output_file, meta):
  lyrics_lst = [meta.get('description', '')]

  # Get lyrics from captions, if any.
  caption_extensions = ['.zh.vtt', '.zh-Hans.vtt', '.zh-Hant.vtt', '.zh-CN.vtt', '.zh-TW.vtt']
  for ext in caption_extensions:
    caption_file = input_file.with_suffix(ext)
    if caption_file.exists():
      vtt = webvtt.read(caption_file)
      text = '\n'.join(c.text for c in vtt.captions)
      lyrics_lst.append(text)
      break  # we can stop after processing the first caption file

  lyrics = '\n\n=====\n\n'.join(lyrics_lst)

  run_process([
    'ffmpeg',
    '-y',
    '-i', str(input_file),
    '-acodec', 'copy',  # copy audio without additional processing
    '-vn',              # ignore video
    ('-ss', meta.get('start')),
    ('-to', meta.get('end')),
    '-metadata', f"genre={meta['genre']}",
    '-metadata', f"title={meta['title']}",
    '-metadata', f"artist={meta['artist']}",
    '-metadata', f"album={meta['album']}",
    '-metadata', f"comment={meta['link']}",
    '-metadata', f"lyrics={lyrics}",
    str(output_file)
  ])

  subprocess.run([
    'aacgain',
    '-r',  # apply Track gain automatically (all files set to equal loudness)
    '-k',  # automatically lower Track/Album gain to not clip audio
    str(output_file)
  ])

  image_file = input_file.with_suffix('.jpg')
  if image_file.exists():
    # Use imagemagick to fix .jpg file so it can be used by AtomicParsley
    subprocess.run(['convert', image_file, image_file])
  else:
    # Use imagemagick to convert .webp file to .jpg file so it can be used by AtomicParsley
    webp_file = input_file.with_suffix('.webp')
    if webp_file.exists():
      subprocess.run(['convert', webp_file, image_file])

  subprocess.run([
    'AtomicParsley',
    str(output_file),
    '--artwork', str(image_file),
    '--overWrite'
  ])
  print(f'\nOutput files generated in {output_file}')

def slurp_json(json_file) = json_file.read_text() |> json.loads

def spit_json(json_file, obj) = json.dumps(obj, indent=2, ensure_ascii=False) |> json_file.write_text

def get_info_objects():
  for info_file in download_dir.glob('*.info.json'):
    info = slurp_json(info_file)
    if info.get('_type') == 'playlist':
      continue
    info['path'] = info_file.parent / info_file.stem |> .with_suffix('.mp4') |> str
    yield info

def run_process(cmd):
  """
  Like subprocess.run, except (key, value) pairs are injected into the list if value is not None (otherwise the pair is
  discarded)
  """
  def reducer(acc, item):
    case item:
      match (key, value):
        return if value is None then acc else acc + list(item)
    else:
      return acc + [item]

  cmd = reduce(reducer, cmd, [])
  print(f'Running command: {cmd}')
  subprocess.run(cmd)

# Entry point
main()
